{"title":"Part 1: A fresh start on the CRNL cluster","markdown":{"yaml":{"title":"Part 1: A fresh start on the CRNL cluster","execute":{"freeze":false,"eval":false},"author":"Romain Ligneul"},"headingText":"Install miniconda to manage your virtual environment","containsRefs":false,"markdown":"\n\nSince we have a brand-new cluster, it is a good time to improve your way of using it if you had always felt that your workflow was not optimal. This tutorial will help you to set up your personal environment if you use Python.\n\n{{< downloadthis www/Tutorial_Cluster_Part1.ipynb dname=\"Tutorial_Cluster_Part1.ipynb\" label=\"Download this notebook\" >}}\n\n:::{.callout-note}\nIf you are working on the CRNL cluster, you can find also the corresponding notebook at this location: `/crnldata/projets_communs/tutorials/`\n:::\n\n\nInstalling miniconda is not incompatible with using venv later on.\n\nOpen a terminal and type the following commands: <br> `cd ~`<br> `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`<br> `bash Miniconda3-latest-Linux-x86_64.sh`<br> `source ~/.bashrc`\n\nFollow the instructions and say yes to everything (you make press Ctrl+C once followed by Enter to skip the text faster)\n\nNB: In JupyterLab, you can open a new terminal at 'File-\\>New-\\>Terminal'. You can then bring this terminal just below your notebook by clicking on its tab and dragging it toward the lower part of the window.\n\n## Create your first conda environment\n\nStill in the terminal, type: <br> `conda create -n crnlenv python=3.9` <br> `conda activate crnlenv`\n\nThe crnlenv virtual environment is active! Everything that will be installed from now on will only be accessible when crnlenv has been activated\n\n## Make your conda environment visible to Jupyter lab\n\nStill in the terminal type: <br> `conda install ipykernel` <br> `python -m ipykernel install --user --name crnlenv --display-name \"Python (crnlenv)\"`\n\nThis commands allow your kernel to be accessed from Jupyter Lab, not only from the command line. If you create more conda environments / kernels, you will also have to run these lines\n\n## Populate your conda environment / kernel with essential tools\n\nInstall a package that allow to submit your jobs easily from any Jupyter notebook on Slurm<br> `conda install -c conda-forge submitit`\n\nInstall numpy <br> `conda install numpy`\n\nInstall a memory_profiler<br> `pip install memory_profiler -U`\n\nLater on you could install various other tools in your virtual environment, but the priority is to check that you can use the cluster and distribute your jobs.\n\nNB: if you wonder why install alternatively with conda or pip, the answer is: you can almost always do it with pip but if it works with conda, the package might be \"better\" installed in some case.\n\n## Let's start computing\n\nYou should be able to see the crnlenv in Jupyterlab if you go in \"Kernel-\\>Change Kernel\".\n\n**Select it** and then restart the kernel (\"Kernel-\\>Restart Kernel\") to continue this tutorial.\n\nOn the top right of this window, you should see something like \"Python (crnlenv)\". It means your notebook is running in the right virtual environment!\n\nFrom now on, you will execute the code cells below, in order. You can do it either by pressing the play button (at the top of the notebook) or by clicking in the target cell and pressing Shift+Enter.\n\nYou may also want to check the tutorials of the module [submitit](https://github.com/facebookincubator/submitit) used here.\n\n```{python}\n###### Import packages/modules\nimport submitit\n# memory profiler to evaluate how much your jobs demand\nfrom memory_profiler import memory_usage\n# import garbage collector: it is sometimes useful to trigger the garbage collector manually with gc.collect()\nimport gc\n# import other modules\nimport time\n```\n\n```{python}\n###### Define a function that should run on the cluster\n\n# this specific function is very dumb and only for demonstration purposes\n# we will just feed it with a number and a string, but we could pass any object to it (filepath, DataFrames, etc.)\n# here, the function only return one argument but it could return several (result result1, result2)\ndef yourFunction(argument1, argument2):\n\n    # print something to the log\n    print('I am running with argument1=' + str(argument1))\n    \n    # sleep for the duration specified by argument1\n    # just to illustrate the parallel processing implemented\n    time.sleep(argument1)\n    \n    # we simply duplicate argument2 as a function of argument1 and return it as our results\n    results=''\n    for i in range(argument1):\n        results=results+'_'+argument2\n\n    # send the results back to the notebook\n    return results\n```\n\n```{python}\n# check time and memory usage of your function\n# ideally, try to test it with the input values that will produce the biggest memory consumption \n# such as the largest file in your dataset or the most fine-grained parameters for your analysis\nstart_time = time.time()\nmem_usage=memory_usage((yourFunction, (3,'consumption',)))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n```\n\n```{python}\n#### Set some environment variables for our jobs\n### for some reason, some default values are set on the cluster, which do not match \n### each other and submitit will complain (this cell might not be needed in the future or on other infrastructures)\nimport os\nos.environ['SLURM_CPUS_PER_TASK'] = '1'\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n```\n\n```{python}\n#### define some array for which each item will be associated with an independent job on the cluster\n#### when you execute these cells, the jobs are sent to the cluster \n\n# here we define an array of numbers: since this array will be used to feed the first argument of yourFunction\n# and that yourFunction waits for as many second as its first argument, the jobs will return in the wrong order\n# (with the output of the second call about 20s after the first one!)\narray_parallel=[1, 20, 2, 5]\n\n# define an additional parameter to be passed to the function\nadditional_parameter='whatever'\n\n# initialize a list in which our returning jobs will be stored\njoblist=[]\n\n# loop over array_parallel\nprint('#### Start submitting jobs #####')\njcount=0\nfor i, value in enumerate(array_parallel):\n    \n  # executor is the submission interface (logs are dumped in the folder)\n  executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n  \n  # set memory, timeout in min, and partition for running the job\n  # if you expect your job to be longer or to require more memory: you will need to increase corresponding values\n  # however, note that increase mem_gb too much is an antisocial selfish behavior :)\n  executor.update_parameters(mem_gb=1, timeout_min=5, slurm_partition=\"CPU\")\n  \n  # actually submit the job: note that \"value\" correspond to that of array_parallel in this iteration\n  job = executor.submit(yourFunction, value, additional_parameter)\n  \n  # add info about job submission order\n  job.job_initial_indice=i \n  \n  # print the ID of your job\n  print(\"submit job\" + str(job.job_id))  \n\n  # append the job to the joblist\n  joblist.append(job)\n\n  # increase the job count\n  jcount=jcount+1\n\n\n### now that the loop has ended we check whether any job is already done\nprint('#### Start waiting for jobs to return #####')\nnjobs_finished = sum(job.done() for job in joblist)\n\n# decide whether we clean our job live or not\nclean_jobs_live=False\n\n# create a list to store finished jobs (optional, and depends on whether we need to cleanup job live)\nfinished_list=[]\nfinished_order=[]\n\n### now we will keep looking for a new finished job until all jobs are done:\nnjobs_finished=0\nwhile njobs_finished<jcount:\n  doneIdx=-1\n  for j, job in enumerate(joblist):\n    if job.done():\n      doneIdx=j\n      break\n  if doneIdx>=0:\n    print(str(1+njobs_finished)+' on ' + str(jcount))\n    # report last job finished\n    print(\"last job finished: \" + job.job_id)\n    # obtain result from job\n    job_result=job.result()\n    # do some processing with this job\n    print(job_result)\n    # decide what to do with the finished job object\n    if clean_jobs_live:\n      # delete the job object\n      del job\n      # collect all the garbage immediately to spare memory\n      gc.collect()\n    else:\n      # if we decided to keep the jobs in a list for further processing, add it finished job list \n      finished_list.append(job)\n      finished_order.append(job.job_initial_indice)\n    # increment the count of finished jobs\n    njobs_finished=njobs_finished+1\n    # remove this finished job from the initial joblist\n    joblist.pop(doneIdx)\n    \nprint('#### All jobs completed #####')\n### If we chose to keep our job results for subsequent processing, it will often be crucial to reorder as a function of their initial\n### submission order, rather than their return order (from the cluster). Here we only keep the results of the job\nif clean_jobs_live==False:\n  finished_results = [finished_list[finished_order[i]].result() for i in finished_order]\n  print('Concatenated results obtained by applying yourFunction() to all items in array_parallel:')\n  print(finished_results)\n```\n\n:::{.callout-warning}\nIf you do not clean your jobs on the fly, then you might saturate the memory of your notebook with `clean_jobs_live=False` because all the job results will be present in the job objects of joblist. \nOnly use this approach if the objects returned by your jobs are light and can be loaded in your limited notebook memory.\nOtherwise, use an approach similar to that implemented by `clean_jobs_live=True`.\n:::\n\n### Next part\n\n[Click here to go to Part 2](./Tutorial_Cluster_Part2.html)\n\n## Comments, questions?\n\nFeel free to comment below to signal a bug, ask a question, etc.\n\n{{< include _comment_box.qmd >}}","srcMarkdownNoYaml":"\n\nSince we have a brand-new cluster, it is a good time to improve your way of using it if you had always felt that your workflow was not optimal. This tutorial will help you to set up your personal environment if you use Python.\n\n{{< downloadthis www/Tutorial_Cluster_Part1.ipynb dname=\"Tutorial_Cluster_Part1.ipynb\" label=\"Download this notebook\" >}}\n\n:::{.callout-note}\nIf you are working on the CRNL cluster, you can find also the corresponding notebook at this location: `/crnldata/projets_communs/tutorials/`\n:::\n\n## Install miniconda to manage your virtual environment\n\nInstalling miniconda is not incompatible with using venv later on.\n\nOpen a terminal and type the following commands: <br> `cd ~`<br> `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`<br> `bash Miniconda3-latest-Linux-x86_64.sh`<br> `source ~/.bashrc`\n\nFollow the instructions and say yes to everything (you make press Ctrl+C once followed by Enter to skip the text faster)\n\nNB: In JupyterLab, you can open a new terminal at 'File-\\>New-\\>Terminal'. You can then bring this terminal just below your notebook by clicking on its tab and dragging it toward the lower part of the window.\n\n## Create your first conda environment\n\nStill in the terminal, type: <br> `conda create -n crnlenv python=3.9` <br> `conda activate crnlenv`\n\nThe crnlenv virtual environment is active! Everything that will be installed from now on will only be accessible when crnlenv has been activated\n\n## Make your conda environment visible to Jupyter lab\n\nStill in the terminal type: <br> `conda install ipykernel` <br> `python -m ipykernel install --user --name crnlenv --display-name \"Python (crnlenv)\"`\n\nThis commands allow your kernel to be accessed from Jupyter Lab, not only from the command line. If you create more conda environments / kernels, you will also have to run these lines\n\n## Populate your conda environment / kernel with essential tools\n\nInstall a package that allow to submit your jobs easily from any Jupyter notebook on Slurm<br> `conda install -c conda-forge submitit`\n\nInstall numpy <br> `conda install numpy`\n\nInstall a memory_profiler<br> `pip install memory_profiler -U`\n\nLater on you could install various other tools in your virtual environment, but the priority is to check that you can use the cluster and distribute your jobs.\n\nNB: if you wonder why install alternatively with conda or pip, the answer is: you can almost always do it with pip but if it works with conda, the package might be \"better\" installed in some case.\n\n## Let's start computing\n\nYou should be able to see the crnlenv in Jupyterlab if you go in \"Kernel-\\>Change Kernel\".\n\n**Select it** and then restart the kernel (\"Kernel-\\>Restart Kernel\") to continue this tutorial.\n\nOn the top right of this window, you should see something like \"Python (crnlenv)\". It means your notebook is running in the right virtual environment!\n\nFrom now on, you will execute the code cells below, in order. You can do it either by pressing the play button (at the top of the notebook) or by clicking in the target cell and pressing Shift+Enter.\n\nYou may also want to check the tutorials of the module [submitit](https://github.com/facebookincubator/submitit) used here.\n\n```{python}\n###### Import packages/modules\nimport submitit\n# memory profiler to evaluate how much your jobs demand\nfrom memory_profiler import memory_usage\n# import garbage collector: it is sometimes useful to trigger the garbage collector manually with gc.collect()\nimport gc\n# import other modules\nimport time\n```\n\n```{python}\n###### Define a function that should run on the cluster\n\n# this specific function is very dumb and only for demonstration purposes\n# we will just feed it with a number and a string, but we could pass any object to it (filepath, DataFrames, etc.)\n# here, the function only return one argument but it could return several (result result1, result2)\ndef yourFunction(argument1, argument2):\n\n    # print something to the log\n    print('I am running with argument1=' + str(argument1))\n    \n    # sleep for the duration specified by argument1\n    # just to illustrate the parallel processing implemented\n    time.sleep(argument1)\n    \n    # we simply duplicate argument2 as a function of argument1 and return it as our results\n    results=''\n    for i in range(argument1):\n        results=results+'_'+argument2\n\n    # send the results back to the notebook\n    return results\n```\n\n```{python}\n# check time and memory usage of your function\n# ideally, try to test it with the input values that will produce the biggest memory consumption \n# such as the largest file in your dataset or the most fine-grained parameters for your analysis\nstart_time = time.time()\nmem_usage=memory_usage((yourFunction, (3,'consumption',)))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n```\n\n```{python}\n#### Set some environment variables for our jobs\n### for some reason, some default values are set on the cluster, which do not match \n### each other and submitit will complain (this cell might not be needed in the future or on other infrastructures)\nimport os\nos.environ['SLURM_CPUS_PER_TASK'] = '1'\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n```\n\n```{python}\n#### define some array for which each item will be associated with an independent job on the cluster\n#### when you execute these cells, the jobs are sent to the cluster \n\n# here we define an array of numbers: since this array will be used to feed the first argument of yourFunction\n# and that yourFunction waits for as many second as its first argument, the jobs will return in the wrong order\n# (with the output of the second call about 20s after the first one!)\narray_parallel=[1, 20, 2, 5]\n\n# define an additional parameter to be passed to the function\nadditional_parameter='whatever'\n\n# initialize a list in which our returning jobs will be stored\njoblist=[]\n\n# loop over array_parallel\nprint('#### Start submitting jobs #####')\njcount=0\nfor i, value in enumerate(array_parallel):\n    \n  # executor is the submission interface (logs are dumped in the folder)\n  executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n  \n  # set memory, timeout in min, and partition for running the job\n  # if you expect your job to be longer or to require more memory: you will need to increase corresponding values\n  # however, note that increase mem_gb too much is an antisocial selfish behavior :)\n  executor.update_parameters(mem_gb=1, timeout_min=5, slurm_partition=\"CPU\")\n  \n  # actually submit the job: note that \"value\" correspond to that of array_parallel in this iteration\n  job = executor.submit(yourFunction, value, additional_parameter)\n  \n  # add info about job submission order\n  job.job_initial_indice=i \n  \n  # print the ID of your job\n  print(\"submit job\" + str(job.job_id))  \n\n  # append the job to the joblist\n  joblist.append(job)\n\n  # increase the job count\n  jcount=jcount+1\n\n\n### now that the loop has ended we check whether any job is already done\nprint('#### Start waiting for jobs to return #####')\nnjobs_finished = sum(job.done() for job in joblist)\n\n# decide whether we clean our job live or not\nclean_jobs_live=False\n\n# create a list to store finished jobs (optional, and depends on whether we need to cleanup job live)\nfinished_list=[]\nfinished_order=[]\n\n### now we will keep looking for a new finished job until all jobs are done:\nnjobs_finished=0\nwhile njobs_finished<jcount:\n  doneIdx=-1\n  for j, job in enumerate(joblist):\n    if job.done():\n      doneIdx=j\n      break\n  if doneIdx>=0:\n    print(str(1+njobs_finished)+' on ' + str(jcount))\n    # report last job finished\n    print(\"last job finished: \" + job.job_id)\n    # obtain result from job\n    job_result=job.result()\n    # do some processing with this job\n    print(job_result)\n    # decide what to do with the finished job object\n    if clean_jobs_live:\n      # delete the job object\n      del job\n      # collect all the garbage immediately to spare memory\n      gc.collect()\n    else:\n      # if we decided to keep the jobs in a list for further processing, add it finished job list \n      finished_list.append(job)\n      finished_order.append(job.job_initial_indice)\n    # increment the count of finished jobs\n    njobs_finished=njobs_finished+1\n    # remove this finished job from the initial joblist\n    joblist.pop(doneIdx)\n    \nprint('#### All jobs completed #####')\n### If we chose to keep our job results for subsequent processing, it will often be crucial to reorder as a function of their initial\n### submission order, rather than their return order (from the cluster). Here we only keep the results of the job\nif clean_jobs_live==False:\n  finished_results = [finished_list[finished_order[i]].result() for i in finished_order]\n  print('Concatenated results obtained by applying yourFunction() to all items in array_parallel:')\n  print(finished_results)\n```\n\n:::{.callout-warning}\nIf you do not clean your jobs on the fly, then you might saturate the memory of your notebook with `clean_jobs_live=False` because all the job results will be present in the job objects of joblist. \nOnly use this approach if the objects returned by your jobs are light and can be loaded in your limited notebook memory.\nOtherwise, use an approach similar to that implemented by `clean_jobs_live=True`.\n:::\n\n### Next part\n\n[Click here to go to Part 2](./Tutorial_Cluster_Part2.html)\n\n## Comments, questions?\n\nFeel free to comment below to signal a bug, ask a question, etc.\n\n{{< include _comment_box.qmd >}}"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"0":"!./www/*","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"Tutorial_Cluster_Part1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.1","theme":["cerulean","custom.scss"],"title":"Part 1: A fresh start on the CRNL cluster","author":"Romain Ligneul"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}