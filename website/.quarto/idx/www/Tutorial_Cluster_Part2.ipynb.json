{"title":"Part 2: Good practices","markdown":{"yaml":{"title":"Part 2: Good practices","format":"html","execute":{"freeze":false,"eval":false},"author":"Romain Ligneul"},"headingText":"Sharing the resources","containsRefs":false,"markdown":"\n\n:::{.callout-note}\nIf you are working on the CRNL cluster, you can find also the corresponding notebook at this location: `/crnldata/projets_communs/tutorials/`\n:::\n\nIf you managed to complete the first part of this tutorial, you will also be able to `pip install` whatever in your virtual environment and do some computing.\nHowever, there is more to know.\n\n\n\nBecause it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\n\nIf you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\n\nThat why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust `mem_gb` and `timeout_min` (timeout in minutes) well.\n\nSimilarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\n\nWhat follows should help you with all this.\n\n### Anticipating time & memory consumption\n\nHereafter, we use memory_usage() which has a slighty unusual way of passing arguments to its target function.\nAll positional arguments (those without an = sign in the *def*) are passed together, and all non-positional arguments (also called key-value pairs) are passed together.\nFor example, we could try:<br>\n`mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))` <br>\nIf we had a function defined like this: <br>\n`somefunc(a,b,c, file=None, index=-1)`\n\n### Evaluating CPU count needs\n\nHow to evaluate whether our job will benefit from having more CPU available to them?\nIf you don't know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\n\nLet's try first with our last function. It should take about 10s to run.\n\nNow let's redo exactly the same thing, with a numpy function that may benefit from multiple CPUs (i.e. np.dot).\n\n### Scaling up **responsibly**\n\nIn the loop above, you might have noticed something new: we've implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\n\n**Number of CPUs**: you can get this information by running `sinfo -o%C` in your terminal, or `!sinfo -o%C` in the notebook. The CPU partitions have about 350 cores available at the time of writing\n**Amount of memory**: you can see this by running `sinfo -o \"%P %n %m\"` in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\n\nIf it is a sunday and nobody is using the cluster, it is probably fine to increase `maxjobs` to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your `maxjobs`-like parameter on a daily basis after checking `sinfo -o%C`.\n\n### A more compact approach\n\nIn the above examples, we have decomposed most operations using `for` loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used.\n\nThe example below (adapted from [submitit documentation](https://github.com/facebookincubator/submitit/blob/main/docs/examples.md)) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations.\nNote that such compact approach might be more difficult to debug.\n\n### Submitting and going home\n\nOften, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\n\nIn this case, we **should not** expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\n\nThis is what we illustrate in the final example below.\n\nIt is a good practice to run `os.system('kill ' + str(os.getpid()))` if you don't need to use the notebook anymore. Simply closing it may not interrupt the process and free the resources for the other users.\n\n## Conclusion\n\nWhether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use. \n\nIf you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\n\nIn any case, it is a good idea to run `os.system('kill ' + str(os.getpid()))`\n\n## Comments, questions?\n\nFeel free to comment, ask questions or report bugs below.\n\n","srcMarkdownNoYaml":"\n\n:::{.callout-note}\nIf you are working on the CRNL cluster, you can find also the corresponding notebook at this location: `/crnldata/projets_communs/tutorials/`\n:::\n\nIf you managed to complete the first part of this tutorial, you will also be able to `pip install` whatever in your virtual environment and do some computing.\nHowever, there is more to know.\n\n\n## Sharing the resources\n\nBecause it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\n\nIf you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\n\nThat why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust `mem_gb` and `timeout_min` (timeout in minutes) well.\n\nSimilarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\n\nWhat follows should help you with all this.\n\n### Anticipating time & memory consumption\n\nHereafter, we use memory_usage() which has a slighty unusual way of passing arguments to its target function.\nAll positional arguments (those without an = sign in the *def*) are passed together, and all non-positional arguments (also called key-value pairs) are passed together.\nFor example, we could try:<br>\n`mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))` <br>\nIf we had a function defined like this: <br>\n`somefunc(a,b,c, file=None, index=-1)`\n\n### Evaluating CPU count needs\n\nHow to evaluate whether our job will benefit from having more CPU available to them?\nIf you don't know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\n\nLet's try first with our last function. It should take about 10s to run.\n\nNow let's redo exactly the same thing, with a numpy function that may benefit from multiple CPUs (i.e. np.dot).\n\n### Scaling up **responsibly**\n\nIn the loop above, you might have noticed something new: we've implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\n\n**Number of CPUs**: you can get this information by running `sinfo -o%C` in your terminal, or `!sinfo -o%C` in the notebook. The CPU partitions have about 350 cores available at the time of writing\n**Amount of memory**: you can see this by running `sinfo -o \"%P %n %m\"` in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\n\nIf it is a sunday and nobody is using the cluster, it is probably fine to increase `maxjobs` to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your `maxjobs`-like parameter on a daily basis after checking `sinfo -o%C`.\n\n### A more compact approach\n\nIn the above examples, we have decomposed most operations using `for` loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used.\n\nThe example below (adapted from [submitit documentation](https://github.com/facebookincubator/submitit/blob/main/docs/examples.md)) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations.\nNote that such compact approach might be more difficult to debug.\n\n### Submitting and going home\n\nOften, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\n\nIn this case, we **should not** expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\n\nThis is what we illustrate in the final example below.\n\nIt is a good practice to run `os.system('kill ' + str(os.getpid()))` if you don't need to use the notebook anymore. Simply closing it may not interrupt the process and free the resources for the other users.\n\n## Conclusion\n\nWhether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use. \n\nIf you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\n\nIn any case, it is a good idea to run `os.system('kill ' + str(os.getpid()))`\n\n## Comments, questions?\n\nFeel free to comment, ask questions or report bugs below.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"0":"!./www/*","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"Tutorial_Cluster_Part2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.1","theme":["cerulean","custom.scss"],"title":"Part 2: Good practices","author":"Romain Ligneul"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}