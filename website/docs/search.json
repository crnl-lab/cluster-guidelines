[
  {
    "objectID": "www/Tutorial_Cluster_Part2.html",
    "href": "www/Tutorial_Cluster_Part2.html",
    "title": "Part 2: Good practices",
    "section": "",
    "text": "Note\n\n\n\nIf you are working on the CRNL cluster, you can find also the corresponding notebook at this location: /crnldata/projets_communs/tutorials/\nIf you managed to complete the first part of this tutorial, you will also be able to pip install whatever in your virtual environment and do some computing. However, there is more to know."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part2.html#sharing-the-resources",
    "href": "www/Tutorial_Cluster_Part2.html#sharing-the-resources",
    "title": "Part 2: Good practices",
    "section": "Sharing the resources",
    "text": "Sharing the resources\nBecause it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\nIf you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\nThat why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust mem_gb and timeout_min (timeout in minutes) well.\nSimilarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\nWhat follows should help you with all this.\n\nAnticipating time & memory consumption\nHereafter, we use memory_usage() which has a slighty unusual way of passing arguments to its target function. All positional arguments (those without an = sign in the def) are passed together, and all non-positional arguments (also called key-value pairs) are passed together. For example, we could try: mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))  If we had a function defined like this:  somefunc(a,b,c, file=None, index=-1)\n\n###### simple memory/time check\nfrom memory_profiler import memory_usage\nimport time\n\n# define a single thread function\ndef duplicate_ones(a, n=100, x=0):\n    import time\n    time.sleep(1)\n    b = [a] * n\n    b = [a] * n\n    b = [a] * n\n    time.sleep(1)\n    return b\n\n# duplicate ones a million time\nprint('Duplicate ones a thousand times')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e3)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n# duplicate ones 100 million times\nprint('Duplicate ones a million time')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e8)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\nprint('Do you notice the difference in time and memory due to the change in duplication size?')\n\n\n\nEvaluating CPU count needs\nHow to evaluate whether our job will benefit from having more CPU available to them? If you don‚Äôt know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\nLet‚Äôs try first with our last function. It should take about 10s to run.\n\nimport os \nimport submitit\n\n# these commands may not be necessary but helped overcoming an error initially\nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n    \n# cpu counts to test\nnCPUs_totest=[1, 4]\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    job = executor.submit(duplicate_ones, 1, int(1e8))\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = sum(job.done() for job in joblist)\nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('### Do you think that increasing the number of CPUs made a big difference? ###')\n\nNow let‚Äôs redo exactly the same thing, with a numpy function that may benefit from multiple CPUs (i.e.¬†np.dot).\n\nimport numpy as np\nimport time\n\ndef mat_multiply(size):\n  # Generate large random matrices\n  A = np.random.rand(size, size)\n  B = np.random.rand(size, size)\n\n  # Measure time for matrix multiplication\n  C = np.dot(A, B)\n  \n  return 'this function does not return anything special'\n  \nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n# cpu counts to test\nnCPUs_totest=[4, 4, 4, 1]\n\n# define the max number of jobs that may run in parallel\nmaxjobs=2\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    # check how many job are running (not done) and wait it they exceed our limit\n    while sum(not job.done() for job in joblist)&gt;maxjobs:\n        print('wait to submit new job')\n        time.sleep(3)\n    job = executor.submit(mat_multiply, 8000)\n    time.sleep(0.5)\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = 0; \nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished and print stats\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        print(\"job status: \" + job.state)\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('\\n### Do you think that increasing the number of CPUs made a big difference? ###')\n\n\n\nScaling up responsibly\nIn the loop above, you might have noticed something new: we‚Äôve implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\nNumber of CPUs: you can get this information by running sinfo -o%C in your terminal, or !sinfo -o%C in the notebook. The CPU partitions have about 350 cores available at the time of writing Amount of memory: you can see this by running sinfo -o \"%P %n %m\" in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\nIf it is a sunday and nobody is using the cluster, it is probably fine to increase maxjobs to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your maxjobs-like parameter on a daily basis after checking sinfo -o%C.\n\n# check node and CPU information\nprint(\"### Node counts: \\nA: currently in use \\B available\")\n!sinfo -o%A\nprint(\"### CPU counts: \\nA: core currently in use \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n!sinfo -o%C\n\n# check some stats of our last job\nprint('### CPU time and MaxRSS of our last job (about 1000Mb should be added to your MaxRSS (Mb) in order to cover safely the memory needs of the python runtime)###')\nos.system(f'sacct -j {job.job_id} --format=\"CPUTime,MaxRSS\"')\n\n\n\nA more compact approach\nIn the above examples, we have decomposed most operations using for loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used.\nThe example below (adapted from submitit documentation) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations. Note that such compact approach might be more difficult to debug.\n\nimport asyncio\n\n# just add a/b, multiply by c and wait for b seconds\ndef simple_function(a, b, c):\n    output=(a + b)*c\n    time.sleep(b)\n    return output\n\n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[0.1]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# add the maxjobs argument\nexecutor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function, a, b, c)  # just a list of jobs\n\n# print results as they become available\nfor aws in asyncio.as_completed([j.awaitable().result() for j in jobs]):\n    result = await aws\n    print(\"result of computation: \" + str(result))\n    arameter \"slurm_array_parallelism\" tells submitit to limit the number of concurrent jobs\nexecutor.\n# note that we use here an asynchronous method based on asyncio\n# it essential do something similar to what we were doing after \n# \"# wait for job completion\", but in a much more compact way\n# however, the reordering of outputs wrt to inputs is not implemented\n\n\n\nSubmitting and going home\nOften, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\nIn this case, we should not expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\nThis is what we illustrate in the final example below.\n\n# write in job_output within our directory\njob_output_folder=os.getcwd()+'/tuto_output/'\n\n# make sure our output folder exists\nif not os.path.exists(job_output_folder):\n  os.makedirs(job_output_folder)\n\n# just add a/b, multiply by c, wait for b seconds and write down the result to an output folder (c)\ndef simple_function_write(a, b, c):\n    output=(a + b)\n    time.sleep(b)\n    output_filepath=os.path.join(c, str(a) + '_' + str(b) + '.txt')\n    with open(output_filepath, 'w') as file:\n      file.write(f'{a}\\n')\n      file.write(f'{b}\\n')\n    \n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[job_output_folder]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=\"tuto_logs\")\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# pass parameter to the executor\nexecutor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function_write, a, b, c)  # just a list of jobs\nprint('### all jobs submitted ###')\nprint('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\nprint('crucially, the content of tuto_output/ will still be updated in the background!')\n\n# wait a little and kill manually the kernel process\ntime.sleep(3)\nos.system('kill ' + str(os.getpid()))\n\nIt is a good practice to run os.system('kill ' + str(os.getpid())) if you don‚Äôt need to use the notebook anymore. Simply closing it may not interrupt the process and free the resources for the other users."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part2.html#conclusion",
    "href": "www/Tutorial_Cluster_Part2.html#conclusion",
    "title": "Part 2: Good practices",
    "section": "Conclusion",
    "text": "Conclusion\nWhether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use.\nIf you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\nIn any case, it is a good idea to run os.system('kill ' + str(os.getpid()))"
  },
  {
    "objectID": "www/Tutorial_Cluster_Part2.html#comments-questions",
    "href": "www/Tutorial_Cluster_Part2.html#comments-questions",
    "title": "Part 2: Good practices",
    "section": "Comments, questions?",
    "text": "Comments, questions?\nFeel free to comment, ask questions or report bugs below."
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html",
    "href": "Tutorial_Cluster_Part2.html",
    "title": "Part 2: Good practices",
    "section": "",
    "text": "Download this notebook\nIf you managed to complete the first part of this tutorial, you will also be able to pip install whatever in your virtual environment and do some computing. However, there is more to know."
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html#sharing-the-resources",
    "href": "Tutorial_Cluster_Part2.html#sharing-the-resources",
    "title": "Part 2: Good practices",
    "section": "Sharing the resources",
    "text": "Sharing the resources\nBecause it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\nIf you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\nThat why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust mem_gb and timeout_min (timeout in minutes) well.\nSimilarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\nWhat follows should help you with all this.\n\nAnticipating time & memory consumption\nHereafter, we use memory_usage() which has a slighty unusual way of passing arguments to its target function. All positional arguments (those without an = sign in the def) are passed together, and all non-positional arguments (also called key-value pairs) are passed together. For example, we could try: mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))  If we had a function defined like this:  somefunc(a,b,c, file=None, index=-1)\n\n###### simple memory/time check\nfrom memory_profiler import memory_usage\nimport time\n\n# define a single thread function\ndef duplicate_ones(a, n=100, x=0):\n    import time\n    time.sleep(1)\n    b = [a] * n\n    b = [a] * n\n    b = [a] * n\n    time.sleep(1)\n    return b\n\n# duplicate ones a million time\nprint('Duplicate ones a thousand times')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e3)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n# duplicate ones 100 million times\nprint('Duplicate ones a million time')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e8)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\nprint('Do you notice the difference in time and memory due to the change in duplication size?')\n\n\n\nEvaluating CPU count needs\nHow to evaluate whether our job will benefit from having more CPU available to them? If you don‚Äôt know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\nLet‚Äôs try first with our last function. It should take about 10s to run.\n\nimport os \nimport submitit\n\n# these commands may not be necessary but helped overcoming an error initially\nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n    \n# cpu counts to test\nnCPUs_totest=[1, 4]\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    job = executor.submit(duplicate_ones, 1, int(1e8))\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = sum(job.done() for job in joblist)\nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('### Do you think that increasing the number of CPUs made a big difference? ###')\n\nNow let‚Äôs redo exactly the same thing, with a numpy function that may benefit from multiple CPUs (i.e.¬†np.dot).\n\nimport numpy as np\nimport time\n\ndef mat_multiply(size):\n  # Generate large random matrices\n  A = np.random.rand(size, size)\n  B = np.random.rand(size, size)\n\n  # Measure time for matrix multiplication\n  C = np.dot(A, B)\n  \n  return 'this function does not return anything special'\n  \nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n# cpu counts to test\nnCPUs_totest=[4, 4, 4, 1]\n\n# define the max number of jobs that may run in parallel\nmaxjobs=2\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    # check how many job are running (not done) and wait it they exceed our limit\n    while sum(not job.done() for job in joblist)&gt;maxjobs:\n        print('wait to submit new job')\n        time.sleep(3)\n    job = executor.submit(mat_multiply, 8000)\n    time.sleep(0.5)\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = 0; \nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished and print stats\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        print(\"job status: \" + job.state)\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('\\n### Do you think that increasing the number of CPUs made a big difference? ###')\n\n\n\nScaling up responsibly\nIn the loop above, you might have noticed something new: we‚Äôve implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\nNumber of CPUs: you can get this information by running sinfo -o%C in your terminal, or !sinfo -o%C in the notebook. The CPU partitions have about 350 cores available at the time of writing Amount of memory: you can see this by running sinfo -o \"%P %n %m\" in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\nIf it is a sunday and nobody is using the cluster, it is probably fine to increase maxjobs to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your maxjobs-like parameter on a daily basis after checking sinfo -o%C.\n\n# check node and CPU information\nprint(\"### Node counts: \\nA: currently in use \\B available\")\n!sinfo -o%A\nprint(\"### CPU counts: \\nA: core currently in use \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n!sinfo -o%C\n\n# check some stats of our last job\nprint('### CPU time and MaxRSS of our last job (about 1000Mb should be added to your MaxRSS (Mb) in order to cover safely the memory needs of the python runtime)###')\nos.system(f'sacct -j {job.job_id} --format=\"CPUTime,MaxRSS\"')\n\n\n\nA more compact approach\nIn the above examples, we have decomposed most operations using for loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used.\nThe example below (adapted from submitit documentation) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations. Note that such compact approach might be more difficult to debug.\n\nimport asyncio\n\n# just add a/b, multiply by c and wait for b seconds\ndef simple_function(a, b, c):\n    output=(a + b)*c\n    time.sleep(b)\n    return output\n\n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[0.1]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# add the maxjobs argument\nexecutor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function, a, b, c)  # just a list of jobs\n\n# print results as they become available\nfor aws in asyncio.as_completed([j.awaitable().result() for j in jobs]):\n    result = await aws\n    print(\"result of computation: \" + str(result))\n    arameter \"slurm_array_parallelism\" tells submitit to limit the number of concurrent jobs\nexecutor.\n# note that we use here an asynchronous method based on asyncio\n# it essential do something similar to what we were doing after \n# \"# wait for job completion\", but in a much more compact way\n# however, the reordering of outputs wrt to inputs is not implemented\n\n\n\nSubmitting and going home\nOften, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\nIn this case, we should not expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\nThis is what we illustrate in the final example below.\n\n# write in job_output within our directory\njob_output_folder=os.getcwd()+'/tuto_output/'\n\n# make sure our output folder exists\nif not os.path.exists(job_output_folder):\n  os.makedirs(job_output_folder)\n\n# just add a/b, multiply by c, wait for b seconds and write down the result to an output folder (c)\ndef simple_function_write(a, b, c):\n    output=(a + b)\n    time.sleep(b)\n    output_filepath=os.path.join(c, str(a) + '_' + str(b) + '.txt')\n    with open(output_filepath, 'w') as file:\n      file.write(f'{a}\\n')\n      file.write(f'{b}\\n')\n    \n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[job_output_folder]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=\"tuto_logs\")\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# pass parameter to the executor\nexecutor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function_write, a, b, c)  # just a list of jobs\nprint('### all jobs submitted ###')\nprint('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\nprint('crucially, the content of tuto_output/ will still be updated in the background!')\n\n# wait a little and kill manually the kernel process\ntime.sleep(3)\nos.system('kill ' + str(os.getpid()))\n\nIt is a good practice to run os.system('kill ' + str(os.getpid())) if you don‚Äôt need to use the notebook anymore. Simply closing it may not interrupt the process and free the resources for the other users."
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html#conclusion",
    "href": "Tutorial_Cluster_Part2.html#conclusion",
    "title": "Part 2: Good practices",
    "section": "Conclusion",
    "text": "Conclusion\nWhether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use.\nIf you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\nIn any case, it is a good idea to run os.system('kill ' + str(os.getpid()))"
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html#comments-questions",
    "href": "Tutorial_Cluster_Part2.html#comments-questions",
    "title": "Part 2: Good practices",
    "section": "Comments, questions?",
    "text": "Comments, questions?\nFeel free to comment, ask questions or report bugs below."
  },
  {
    "objectID": "help_VScode.html",
    "href": "help_VScode.html",
    "title": "VScode on the cluster",
    "section": "",
    "text": "VScode is a very potent IDE that is used by many people, including at the CRNL.\nHowever, the straightforward usage of VScode on the cluster poses a big collective problem. Indeed, when you open a SSH session with VScode, you arrive in the so-called ‚Äúlogin node‚Äù (currently node14 on the new server, and node12 on the old one). This node is not supposed to perform any computation, but instead to organise input-output, ssh connections, etc.\nEven if they do most of their computations using jobs submitted to SLURM, VScode users (I am amongst them) can still make (made!) the entire infrastructure crash by overloading the ‚Äúlogin node‚Äù. This is because every VScode user will open some notebook, load some data, call slurm from that same login node. So, when 10 or 20 of them are connected at the same time‚Ä¶\nüí•\nFortunately, a simple solution exists and this tutorial will guide you through it."
  },
  {
    "objectID": "help_VScode.html#leave-a-message",
    "href": "help_VScode.html#leave-a-message",
    "title": "VScode on the cluster",
    "section": "Leave a message",
    "text": "Leave a message\nFeel free to leave a message."
  },
  {
    "objectID": "help_ssh_jupyter.html",
    "href": "help_ssh_jupyter.html",
    "title": "Using Interactive Jupyter over SSH",
    "section": "",
    "text": "Visual Code over SSH is great but it is limited in its ability to run interactive Jupyter notebooks unless X11 forwarding is properly configured. Another way to go is to run a persistent Jupyter lab session on your remote SSH server and forward it through a tunnel."
  },
  {
    "objectID": "help_ssh_jupyter.html#extend-your-.bashrc-file",
    "href": "help_ssh_jupyter.html#extend-your-.bashrc-file",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Extend your .bashrc file",
    "text": "Extend your .bashrc file\nTo do so, we need to edit our .bashrc file (in /home/username/.bashrc) and add the following lines at the end.\n# &gt;&gt;&gt; Jupyterlab Remote &gt;&gt;&gt;\nfunction jlremote {\n    echo $(hostname) &gt; ~/.jupyternode.txt\n    cd /crnldata/cophy/\n    XDG_RUNTIME_DIR= jupyter lab --no-browser --port=9753 --ip=$(hostname)\n}\n# &lt;&lt;&lt; Jupyterlab end config &lt;&lt;&lt;\nNote that the ‚Äìport variable can be changed to your preference.\nThanks to this piece of code, each time a terminal will open, the function jlremote will be added to the path."
  },
  {
    "objectID": "help_ssh_jupyter.html#launch-the-jupyter-session",
    "href": "help_ssh_jupyter.html#launch-the-jupyter-session",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Launch the Jupyter session",
    "text": "Launch the Jupyter session\nMost remote linux servers will offer the opportunity to run tmux, that allows opening a persistent shell session.\nIn your remote terminal, type tmux. The tmux session will open. That is where we wll run our Jupyter notebook.\nThen, type srun --mem=4G --pty bash. This will associate your tmux session with a compute node on the remote server. You can adjust the memory you need for your notebooks.\nThen, type jlremote. This will launch your Jupyter lab session. It might take a bit of time before displaying a message like this one\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\nCopy the token value (after = sign) and keep it somewhere."
  },
  {
    "objectID": "help_ssh_jupyter.html#set-up-the-ssh-tunnel",
    "href": "help_ssh_jupyter.html#set-up-the-ssh-tunnel",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Set up the SSH tunnel",
    "text": "Set up the SSH tunnel\n\nWindows\nOn your local Windows compute, create a .bat file\n@echo off\nsetlocal\n:: the port you've chosen in the function jlremote \nset port=9753\n:: your username on the remote server\nset remote_username=firstname.lastname\n:: your machine IP\nset remote_hostname=10.69.168.62 \n\nfor /f \"tokens=* usebackq\" %%i in (`powershell -command \"ssh %remote_username%@%remote_hostname% 'tail -1 ~/.jupyternode.txt'\"`) do set node=%%i\n\n:: Read the node from the temporary file\nset /p node=&lt;node.txt\n\nset url=http://localhost:%port%\n\n:: Construct and run the SSH command\nset cmd=ssh -CNL 8888:%node%:%port% %remote_username%@%remote_hostname%\necho Running '%cmd%'\n\n:: Delete the temporary file\ndel node.txt\n\n%cmd%\n\nendlocal\nRun (or double click on) the .bat file.\nYou should be able to use your notebook by using the address provided earlier.\nhttp://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nYou may also just use http://localhost:8888/ and add the token manually.\nYou may set a simple password to access the notebook without the token in the future.\n\n\nLinux\nOn your local Windows compute, create a .bat file\nfunction jllocal {\n    port=9753\n    remote_username=USERNAME\n    remote_hostname=HOSTNAME\n    node=$(ssh $remote_username@$remote_hostname 'tail -1 ~/.jupyternode.txt')\n    url=\"http://localhost:$port\"\n    echo \"Opening $url\"\n    open \"$url\"\n    cmd=\"ssh -CNL \"8888\":\"$node\":\"$port\" $remote_username@$remote_hostname\"\n    echo \"Running '$cmd'\"\n    eval \"$cmd\"\n}\nRun (or double click on) the .bat file.\nYou should be able to use your notebook by using the address provided earlier.\nhttp://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nYou may also just use http://localhost:8888/ and add the token manually .\nYou may set a simple password to access the notebook without the token in the future."
  },
  {
    "objectID": "help_ssh_jupyter.html#credits",
    "href": "help_ssh_jupyter.html#credits",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Credits",
    "text": "Credits\nThis tutorial was largely inspired by this one:\nhttps://benjlindsay.com/posts/running-jupyter-lab-remotely/"
  },
  {
    "objectID": "help_github.html",
    "href": "help_github.html",
    "title": "Quick tutorial for Git/Github",
    "section": "",
    "text": "If you only plan to clone public repositories, you can probably just use git clone https:// or simply download and unzip the repository of interest.\nHowever, you will need to connect your command line to your Github account in the following scenarios:"
  },
  {
    "objectID": "help_github.html#leave-a-message",
    "href": "help_github.html#leave-a-message",
    "title": "Quick tutorial for Git/Github",
    "section": "Leave a message",
    "text": "Leave a message\nFeel free to leave a message."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CRNL cluster guidelines",
    "section": "",
    "text": "This website provides guidelines to use the cluster of the Centre de Recherche en Neurosciences de Lyon (CRNL).\nIt is managed and it has been configured by Thibaut Woog (thibaut.woog@inserm.fr)."
  },
  {
    "objectID": "about.html#using-jupyter-lab",
    "href": "about.html#using-jupyter-lab",
    "title": "CRNL cluster guidelines",
    "section": "Using Jupyter lab",
    "text": "Using Jupyter lab\nOur new cluster has a built-in Jupyterhub system that will greatly facilitate the life of most users and ITs, as it helps managing the load on the ‚Äúentry point‚Äù server (i.e.¬†10.69.168.93). Jupyterhub offers to everyone a very complete IDE for Python and Matlab programming, as well as a Desktop application (similar to former VNC connections).\nYet, you will still need to learn how to create virtual environments (especially Python users) and how to submit jobs efficiently and ethically.\n\nObtain VPN access (optional)\nIf you want to access the cluster remotely, check this page.\n\n\nConfigure your Git (optional but recommended)\n\n\nLearn to submit jobs\nThe cluster uses SLURM to manage ‚Äújobs‚Äù (i.e.¬†functions/scripts running in parallel). Check the two part tutorial for efficient and responsible job submission on the cluster using Slurm and Python (including interactive Notebooks).\nPart 1 Part 2"
  },
  {
    "objectID": "about.html#using-vs-code",
    "href": "about.html#using-vs-code",
    "title": "CRNL cluster guidelines",
    "section": "Using VS code",
    "text": "Using VS code"
  },
  {
    "objectID": "about.html#get-git",
    "href": "about.html#get-git",
    "title": "CRNL cluster guidelines",
    "section": "1- Get Git",
    "text": "1- Get Git\nCheck the tutorial for easy SSH access with SSH keys, to avoid having to enter passwords in the future."
  },
  {
    "objectID": "about.html#set-up-ssh-and-visual-code",
    "href": "about.html#set-up-ssh-and-visual-code",
    "title": "CRNL cluster guidelines",
    "section": "2- Set up SSH and Visual Code",
    "text": "2- Set up SSH and Visual Code\nCheck the tutorial for easy SSH access with SSH keys, to avoid having to enter passwords in the future."
  },
  {
    "objectID": "about.html#how-to-contribute",
    "href": "about.html#how-to-contribute",
    "title": "CRNL cluster guidelines",
    "section": "How to contribute",
    "text": "How to contribute\nTo contribute to the development and conception of these guidelines, request access to the CRNL Github organization to Samuel Garcia or Romain Ligneul."
  },
  {
    "objectID": "about.html#leave-a-message",
    "href": "about.html#leave-a-message",
    "title": "CRNL cluster guidelines",
    "section": "Leave a message",
    "text": "Leave a message\nFeel free to leave a message."
  },
  {
    "objectID": "help_filezilla.html",
    "href": "help_filezilla.html",
    "title": "Managing files on the cluster (Filezilla)",
    "section": "",
    "text": "To transfer and manage folders/files on the cluster, you can use Filezilla.\nSimply install it. Then, in Site Manager, create a new site (give it the name CRNLcluster, for example) and set the following:"
  },
  {
    "objectID": "help_filezilla.html#good-sides-of-filezilla",
    "href": "help_filezilla.html#good-sides-of-filezilla",
    "title": "Managing files on the cluster (Filezilla)",
    "section": "Good sides of Filezilla",
    "text": "Good sides of Filezilla\nWhen using the cluster, you might sometimes struggle with Files and Folders authorizations. Those can be set in the command line with chmod, but Filezilla can do it for you in a more intuitive way. Just right click on any folder and select ‚ÄúFile permissions‚Ä¶‚Äù\n\nAnother good side is that you can apply filters to upload/download files in batch, including or excluding files according to specific criteria (types, sizes, etc.). Check ‚ÄúView -&gt; Directory Listing Filters‚Äù."
  },
  {
    "objectID": "help_filezilla.html#leave-a-message",
    "href": "help_filezilla.html#leave-a-message",
    "title": "Managing files on the cluster (Filezilla)",
    "section": "Leave a message",
    "text": "Leave a message\nFeel free to leave a message."
  },
  {
    "objectID": "help_ssh.html",
    "href": "help_ssh.html",
    "title": "Connecting to the CRNL server over SSH in Visual Code",
    "section": "",
    "text": "Note\n\n\n\nThe steps in this tutorial are not strictly mandatory. They are mainly meant to avoid entering your password to log into the cluster. In the long run they will help you to save a lot of time, if you use VS code, Filezilla or any other SSH based connection.\n\n\nFind your local .ssh folder (local always refer to the computer you are using).\nOn Linux, you can probably find it at ~/.ssh/ or /home/username/.ssh/.\nOn Window, you can probably find at at C:\\Users\\username\\.ssh\\.\nIf you don‚Äôt find it, just create it and cd in it. Then type:\nssh-keygen -f crnlcluster\n\n\n\n\n\n\nNote\n\n\n\nIf ssh-keygen is not found, you may need to install Putty, restart your terminal and perform these steps again.\n\n\nFollow the instructions. If you want to use a passphrase, keep it short. Otherwise press Enter to skip entering a passphrase. Then type: type crnlcluster.pub\nYou public key will appear in the terminal. Keep it open.\nNow, type:\nssh cluster_username@10.69.168.93\nChange cluster_username to match your username on the cluster (typically, firstname.lastname)\nYou will be asked to enter your CRNL password (the same you use to enter in the wiki.crnl.fr and to open Jupyter lab sessions)\nNow, you are logged in the cluster! Type:\ncd ~/.ssh/\ntouch authorized_keys\nvi authorized_keys\nThis last command will open a blank file, unless you already registered some SSH keys in the past. In any case, make sure your pointer is at the end of the document and copy paste what had appeared in your local terminal (after type crnlcluster.pub). To paste, you only need to make a right-click. To save and close the authorized_keys file, just type :wq in it.\nClose the terminal.\nNow, the cluster knows your ssh key and you won‚Äôt need to use the password anymore (only the ssh key passphrase if you entered one). But let‚Äôs make an extra-step to further automatize the process.\nIn your local .ssh folder, create a file named config and add the following lines to it:\nHost CRNLcluster\n  HostName 10.69.168.93\n  User cluster_username\n  ForwardX11 yes\n  ForwardX11Trusted yes\n  IdentityFile \"path/to/your/sshkey/crnlcluster\"\nAdjust cluster_username and do not add the .pub extension to your IdentityFile/sshkey.\nNow, each time you want to reach the command line of the cluster, open a terminal and type:\nssh cluster_username@CRNLcluster\nYou are already into the cluster!"
  },
  {
    "objectID": "help_VPN.html",
    "href": "help_VPN.html",
    "title": "VPN access",
    "section": "",
    "text": "If you only want to use the cluster when you are at the lab, you don‚Äôt need the VPN.\nThe VPN is only required to access the CRNL servers from outside. You can only obtain it if you are a registered CRNL user.\nhttps://wiki.crnl.fr/doku.php?id=wiki:informatique:services:vpn\nYou need your CRNL identifiers to access this internal page. Essentially, besides installing the software, you will have to fill the PDF ‚Äúfiche‚Äù (here) and send it to thibaut.woog@inserm.fr. Then, it will be a matter of days before you can access the CRNL infrastructure remotely. You‚Äôll be noticed by email.\nIf you don‚Äôt have an INSERM email, you can ask one if you work at CRNL (even if you are affiliated to CNRS or UCBL)."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html",
    "href": "Tutorial_Cluster_Part1.html",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "",
    "text": "Since we have a brand-new cluster, it is a good time to improve your way of using it if you had always felt that your workflow was not optimal. This tutorial will help you to set up your personal environment if you use Python.\nDownload this notebook"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "href": "Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Install miniconda to manage your virtual environment",
    "text": "Install miniconda to manage your virtual environment\nInstalling miniconda is not incompatible with using venv later on.\nOpen a terminal and type the following commands:  cd ~ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc\nFollow the instructions and say yes to everything (you make press Ctrl+C once followed by Enter to skip the text faster)\nNB: In JupyterLab, you can open a new terminal at ‚ÄòFile-&gt;New-&gt;Terminal‚Äô. You can then bring this terminal just below your notebook by clicking on its tab and dragging it toward the lower part of the window."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "href": "Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Create your first conda environment",
    "text": "Create your first conda environment\nStill in the terminal, type:  conda create -n crnlenv python=3.9  conda activate crnlenv\nThe crnlenv virtual environment is active! Everything that will be installed from now on will only be accessible when crnlenv has been activated"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "href": "Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Make your conda environment visible to Jupyter lab",
    "text": "Make your conda environment visible to Jupyter lab\nStill in the terminal type:  conda install ipykernel  python -m ipykernel install --user --name crnlenv --display-name \"Python (crnlenv)\"\nThis commands allow your kernel to be accessed from Jupyter Lab, not only from the command line. If you create more conda environments / kernels, you will also have to run these lines"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "href": "Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Populate your conda environment / kernel with essential tools",
    "text": "Populate your conda environment / kernel with essential tools\nInstall a package that allow to submit your jobs easily from any Jupyter notebook on Slurm conda install -c conda-forge submitit\nInstall numpy  conda install numpy\nInstall a memory_profiler pip install memory_profiler -U\nLater on you could install various other tools in your virtual environment, but the priority is to check that you can use the cluster and distribute your jobs.\nNB: if you wonder why install alternatively with conda or pip, the answer is: you can almost always do it with pip but if it works with conda, the package might be ‚Äúbetter‚Äù installed in some case."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#lets-start-computing",
    "href": "Tutorial_Cluster_Part1.html#lets-start-computing",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Let‚Äôs start computing",
    "text": "Let‚Äôs start computing\nYou should be able to see the crnlenv in Jupyterlab if you go in ‚ÄúKernel-&gt;Change Kernel‚Äù.\nSelect it and then restart the kernel (‚ÄúKernel-&gt;Restart Kernel‚Äù) to continue this tutorial.\nOn the top right of this window, you should see something like ‚ÄúPython (crnlenv)‚Äù. It means your notebook is running in the right virtual environment!\nFrom now on, you will execute the code cells below, in order. You can do it either by pressing the play button (at the top of the notebook) or by clicking in the target cell and pressing Shift+Enter.\nYou may also want to check the tutorials of the module submitit used here.\n\n###### Import packages/modules\nimport submitit\n# memory profiler to evaluate how much your jobs demand\nfrom memory_profiler import memory_usage\n# import garbage collector: it is sometimes useful to trigger the garbage collector manually with gc.collect()\nimport gc\n# import other modules\nimport time\n\n\n###### Define a function that should run on the cluster\n\n# this specific function is very dumb and only for demonstration purposes\n# we will just feed it with a number and a string, but we could pass any object to it (filepath, DataFrames, etc.)\n# here, the function only return one argument but it could return several (result result1, result2)\ndef yourFunction(argument1, argument2):\n\n    # print something to the log\n    print('I am running with argument1=' + str(argument1))\n    \n    # sleep for the duration specified by argument1\n    # just to illustrate the parallel processing implemented\n    time.sleep(argument1)\n    \n    # we simply duplicate argument2 as a function of argument1 and return it as our results\n    results=''\n    for i in range(argument1):\n        results=results+'_'+argument2\n\n    # send the results back to the notebook\n    return results\n\n\n# check time and memory usage of your function\n# ideally, try to test it with the input values that will produce the biggest memory consumption \n# such as the largest file in your dataset or the most fine-grained parameters for your analysis\nstart_time = time.time()\nmem_usage=memory_usage((yourFunction, (3,'consumption',)))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n\n#### Set some environment variables for our jobs\n### for some reason, some default values are set on the cluster, which do not match \n### each other and submitit will complain (this cell might not be needed in the future or on other infrastructures)\nimport os\nos.environ['SLURM_CPUS_PER_TASK'] = '1'\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n\n#### define some array for which each item will be associated with an independent job on the cluster\n#### when you execute these cells, the jobs are sent to the cluster \n\n# here we define an array of numbers: since this array will be used to feed the first argument of yourFunction\n# and that yourFunction waits for as many second as its first argument, the jobs will return in the wrong order\n# (with the output of the second call about 20s after the first one!)\narray_parallel=[1, 20, 2, 5]\n\n# define an additional parameter to be passed to the function\nadditional_parameter='whatever'\n\n# initialize a list in which our returning jobs will be stored\njoblist=[]\n\n# loop over array_parallel\nprint('#### Start submitting jobs #####')\njcount=0\nfor i, value in enumerate(array_parallel):\n    \n  # executor is the submission interface (logs are dumped in the folder)\n  executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n  \n  # set memory, timeout in min, and partition for running the job\n  # if you expect your job to be longer or to require more memory: you will need to increase corresponding values\n  # however, note that increase mem_gb too much is an antisocial selfish behavior :)\n  executor.update_parameters(mem_gb=1, timeout_min=5, slurm_partition=\"CPU\")\n  \n  # actually submit the job: note that \"value\" correspond to that of array_parallel in this iteration\n  job = executor.submit(yourFunction, value, additional_parameter)\n  \n  # add info about job submission order\n  job.job_initial_indice=i \n  \n  # print the ID of your job\n  print(\"submit job\" + str(job.job_id))  \n\n  # append the job to the joblist\n  joblist.append(job)\n\n  # increase the job count\n  jcount=jcount+1\n\n\n### now that the loop has ended we check whether any job is already done\nprint('#### Start waiting for jobs to return #####')\nnjobs_finished = sum(job.done() for job in joblist)\n\n# decide whether we clean our job live or not\nclean_jobs_live=False\n\n# create a list to store finished jobs (optional, and depends on whether we need to cleanup job live)\nfinished_list=[]\nfinished_order=[]\n\n### now we will keep looking for a new finished job until all jobs are done:\nnjobs_finished=0\nwhile njobs_finished&lt;jcount:\n  doneIdx=-1\n  for j, job in enumerate(joblist):\n    if job.done():\n      doneIdx=j\n      break\n  if doneIdx&gt;=0:\n    print(str(1+njobs_finished)+' on ' + str(jcount))\n    # report last job finished\n    print(\"last job finished: \" + job.job_id)\n    # obtain result from job\n    job_result=job.result()\n    # do some processing with this job\n    print(job_result)\n    # decide what to do with the finished job object\n    if clean_jobs_live:\n      # delete the job object\n      del job\n      # collect all the garbage immediately to spare memory\n      gc.collect()\n    else:\n      # if we decided to keep the jobs in a list for further processing, add it finished job list \n      finished_list.append(job)\n      finished_order.append(job.job_initial_indice)\n    # increment the count of finished jobs\n    njobs_finished=njobs_finished+1\n    # remove this finished job from the initial joblist\n    joblist.pop(doneIdx)\n    \nprint('#### All jobs completed #####')\n### If we chose to keep our job results for subsequent processing, it will often be crucial to reorder as a function of their initial\n### submission order, rather than their return order (from the cluster). Here we only keep the results of the job\nif clean_jobs_live==False:\n  finished_results = [finished_list[finished_order[i]].result() for i in finished_order]\n  print('Concatenated results obtained by applying yourFunction() to all items in array_parallel:')\n  print(finished_results)\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not clean your jobs on the fly, then you might saturate the memory of your notebook with clean_jobs_live=False because all the job results will be present in the job objects of joblist. Only use this approach if the objects returned by your jobs are light and can be loaded in your limited notebook memory. Otherwise, use an approach similar to that implemented by clean_jobs_live=True.\n\n\n\nNext part\nClick here to go to Part 2"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#comments-questions",
    "href": "Tutorial_Cluster_Part1.html#comments-questions",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Comments, questions?",
    "text": "Comments, questions?\nFeel free to comment below to signal a bug, ask a question, etc."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html",
    "href": "www/Tutorial_Cluster_Part1.html",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "",
    "text": "Since we have a brand-new cluster, it is a good time to improve your way of using it if you had always felt that your workflow was not optimal. This tutorial will help you to set up your personal environment if you use Python."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "href": "www/Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Install miniconda to manage your virtual environment",
    "text": "Install miniconda to manage your virtual environment\nInstalling miniconda is not incompatible with using venv later on.\nOpen a terminal and type the following commands:  cd ~ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc\nFollow the instructions and say yes to everything (you make press Ctrl+C once followed by Enter to skip the text faster)\nNB: In JupyterLab, you can open a new terminal at ‚ÄòFile-&gt;New-&gt;Terminal‚Äô. You can then bring this terminal just below your notebook by clicking on its tab and dragging it toward the lower part of the window."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "href": "www/Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Create your first conda environment",
    "text": "Create your first conda environment\nStill in the terminal, type:  conda create -n crnlenv python=3.9  conda activate crnlenv\nThe crnlenv virtual environment is active! Everything that will be installed from now on will only be accessible when crnlenv has been activated"
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "href": "www/Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Make your conda environment visible to Jupyter lab",
    "text": "Make your conda environment visible to Jupyter lab\nStill in the terminal type:  conda install ipykernel  python -m ipykernel install --user --name crnlenv --display-name \"Python (crnlenv)\"\nThis commands allow your kernel to be accessed from Jupyter Lab, not only from the command line. If you create more conda environments / kernels, you will also have to run these lines"
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "href": "www/Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Populate your conda environment / kernel with essential tools",
    "text": "Populate your conda environment / kernel with essential tools\nInstall a package that allow to submit your jobs easily from any Jupyter notebook on Slurm conda install -c conda-forge submitit\nInstall numpy  conda install numpy\nInstall a memory_profiler pip install memory_profiler -U\nLater on you could install various other tools in your virtual environment, but the priority is to check that you can use the cluster and distribute your jobs.\nNB: if you wonder why install alternatively with conda or pip, the answer is: you can almost always do it with pip but if it works with conda, the package might be ‚Äúbetter‚Äù installed in some case."
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#lets-start-computing",
    "href": "www/Tutorial_Cluster_Part1.html#lets-start-computing",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Let‚Äôs start computing",
    "text": "Let‚Äôs start computing\nYou should be able to see the crnlenv in Jupyterlab if you go in ‚ÄúKernel-&gt;Change Kernel‚Äù.\nSelect it and then restart the kernel (‚ÄúKernel-&gt;Restart Kernel‚Äù) to continue this tutorial.\nOn the top right of this window, you should see something like ‚ÄúPython (crnlenv)‚Äù. It means your notebook is running in the right virtual environment!\nFrom now on, you will execute the code cells below, in order. You can do it either by pressing the play button (at the top of the notebook) or by clicking in the target cell and pressing Shift+Enter.\nYou may also want to check the tutorials of the module submitit used here.\n\n###### Import packages/modules\nimport submitit\n# memory profiler to evaluate how much your jobs demand\nfrom memory_profiler import memory_usage\n# import garbage collector: it is sometimes useful to trigger the garbage collector manually with gc.collect()\nimport gc\n# import other modules\nimport time\n\n\n###### Define a function that should run on the cluster\n\n# this specific function is very dumb and only for demonstration purposes\n# we will just feed it with a number and a string, but we could pass any object to it (filepath, DataFrames, etc.)\n# here, the function only return one argument but it could return several (result result1, result2)\ndef yourFunction(argument1, argument2):\n\n    # print something to the log\n    print('I am running with argument1=' + str(argument1))\n    \n    # sleep for the duration specified by argument1\n    # just to illustrate the parallel processing implemented\n    time.sleep(argument1)\n    \n    # we simply duplicate argument2 as a function of argument1 and return it as our results\n    results=''\n    for i in range(argument1):\n        results=results+'_'+argument2\n\n    # send the results back to the notebook\n    return results\n\n\n# check time and memory usage of your function\n# ideally, try to test it with the input values that will produce the biggest memory consumption \n# such as the largest file in your dataset or the most fine-grained parameters for your analysis\nstart_time = time.time()\nmem_usage=memory_usage((yourFunction, (3,'consumption',)))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n\n#### Set some environment variables for our jobs\n### for some reason, some default values are set on the cluster, which do not match \n### each other and submitit will complain (this cell might not be needed in the future or on other infrastructures)\nimport os\nos.environ['SLURM_CPUS_PER_TASK'] = '1'\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n\n#### define some array for which each item will be associated with an independent job on the cluster\n#### when you execute these cells, the jobs are sent to the cluster \n\n# here we define an array of numbers: since this array will be used to feed the first argument of yourFunction\n# and that yourFunction waits for as many second as its first argument, the jobs will return in the wrong order\n# (with the output of the second call about 20s after the first one!)\narray_parallel=[1, 20, 2, 5]\n\n# define an additional parameter to be passed to the function\nadditional_parameter='whatever'\n\n# initialize a list in which our returning jobs will be stored\njoblist=[]\n\n# loop over array_parallel\nprint('#### Start submitting jobs #####')\njcount=0\nfor i, value in enumerate(array_parallel):\n    \n  # executor is the submission interface (logs are dumped in the folder)\n  executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n  \n  # set memory, timeout in min, and partition for running the job\n  # if you expect your job to be longer or to require more memory: you will need to increase corresponding values\n  # however, note that increase mem_gb too much is an antisocial selfish behavior :)\n  executor.update_parameters(mem_gb=1, timeout_min=5, slurm_partition=\"CPU\")\n  \n  # actually submit the job: note that \"value\" correspond to that of array_parallel in this iteration\n  job = executor.submit(yourFunction, value, additional_parameter)\n  \n  # add info about job submission order\n  job.job_initial_indice=i \n  \n  # print the ID of your job\n  print(\"submit job\" + str(job.job_id))  \n\n  # append the job to the joblist\n  joblist.append(job)\n\n  # increase the job count\n  jcount=jcount+1\n\n\n### now that the loop has ended we check whether any job is already done\nprint('#### Start waiting for jobs to return #####')\nnjobs_finished = sum(job.done() for job in joblist)\n\n# decide whether we clean our job live or not\nclean_jobs_live=False\n\n# create a list to store finished jobs (optional, and depends on whether we need to cleanup job live)\nfinished_list=[]\nfinished_order=[]\n\n### now we will keep looking for a new finished job until all jobs are done:\nnjobs_finished=0\nwhile njobs_finished&lt;jcount:\n  doneIdx=-1\n  for j, job in enumerate(joblist):\n    if job.done():\n      doneIdx=j\n      break\n  if doneIdx&gt;=0:\n    print(str(1+njobs_finished)+' on ' + str(jcount))\n    # report last job finished\n    print(\"last job finished: \" + job.job_id)\n    # obtain result from job\n    job_result=job.result()\n    # do some processing with this job\n    print(job_result)\n    # decide what to do with the finished job object\n    if clean_jobs_live:\n      # delete the job object\n      del job\n      # collect all the garbage immediately to spare memory\n      gc.collect()\n    else:\n      # if we decided to keep the jobs in a list for further processing, add it finished job list \n      finished_list.append(job)\n      finished_order.append(job.job_initial_indice)\n    # increment the count of finished jobs\n    njobs_finished=njobs_finished+1\n    # remove this finished job from the initial joblist\n    joblist.pop(doneIdx)\n    \nprint('#### All jobs completed #####')\n### If we chose to keep our job results for subsequent processing, it will often be crucial to reorder as a function of their initial\n### submission order, rather than their return order (from the cluster). Here we only keep the results of the job\nif clean_jobs_live==False:\n  finished_results = [finished_list[finished_order[i]].result() for i in finished_order]\n  print('Concatenated results obtained by applying yourFunction() to all items in array_parallel:')\n  print(finished_results)\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not clean your jobs on the fly, then you might saturate the memory of your notebook with clean_jobs_live=False because all the job results will be present in the job objects of joblist. Only use this approach if the objects returned by your jobs are light and can be loaded in your limited notebook memory. Otherwise, use an approach similar to that implemented by clean_jobs_live=True.\n\n\n\nNext part\nClick here to go to Part 2"
  },
  {
    "objectID": "www/Tutorial_Cluster_Part1.html#comments-questions",
    "href": "www/Tutorial_Cluster_Part1.html#comments-questions",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Comments, questions?",
    "text": "Comments, questions?\nFeel free to comment below to signal a bug, ask a question, etc."
  }
]